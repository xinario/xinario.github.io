<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.7.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Xin Yi">

  
  
  
    
  
  <meta name="description" content="Software R&amp;D Engineer">

  
  <link rel="alternate" hreflang="en-us" href="/publication/">

  


  
  
  
  <meta name="theme-color" content="#0095eb">
  

  
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      
        
      

      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  
  <link rel="alternate" href="/publication/index.xml" type="application/rss+xml" title="XIN YI">
  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="/publication/">

  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="XIN YI">
  <meta property="og:url" content="/publication/">
  <meta property="og:title" content="Publications | XIN YI">
  <meta property="og:description" content="Software R&amp;D Engineer"><meta property="og:image" content="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png">
  <meta property="twitter:image" content="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us">
  
    <meta property="og:updated_time" content="2020-02-01T00:00:00&#43;00:00">
  

  




  


  





  <title>Publications | XIN YI</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">XIN YI</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">XIN YI</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications"><span>Publications</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item">
        <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
      </li>
      

      

    </ul>

  </div>
</nav>


  












  

  
  
  
    
  
<div class="universal-wrapper pt-3">
  <h1>Publications</h1>

  

  
</div>



<div class="universal-wrapper">
  <div class="row">
    <div class="col-lg-12">

      

      
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      

      <div class="form-row mb-4">
        <div class="col-auto">
          <input type="search" class="filter-search" placeholder="Search..." autocapitalize="off"
          autocomplete="off" autocorrect="off" role="textbox" spellcheck="false">
        </div>
        <div class="col-auto">
          <select class="pub-filters pubtype-select form-control form-control-sm" data-filter-group="pubtype">
            <option value="*">Type</option>
            
            
            <option value=".pubtype-1">
              Conference paper
            </option>
            
            <option value=".pubtype-2">
              Journal article
            </option>
            
            <option value=".pubtype-3">
              Preprint
            </option>
            
          </select>
        </div>
        <div class="col-auto">
          <select class="pub-filters form-control form-control-sm" data-filter-group="year">
            <option value="*">Date</option>
            
            
            
            <option value=".year-2020">
              2020
            </option>
            
            <option value=".year-2019">
              2019
            </option>
            
            <option value=".year-2018">
              2018
            </option>
            
            <option value=".year-2016">
              2016
            </option>
            
            <option value=".year-2014">
              2014
            </option>
            
            
          </select>
        </div>
      </div>

      <div id="container-publications">
        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    



  
  <span><a href="/authors/xin-yi/">Xin Yi</a></span>, <span><a href="/authors/scott-adams/">Scott Adams</a></span>, <span><a href="/authors/robert-henderson/">Robert Henderson</a></span>, <span><a href="/authors/paul-babyn/">Paul Babyn</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    February 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Radiology AI</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/journal-article/catheter_review/">Computer-aided Assessment of Catheters and Tubes onRadiographs: How Good Is Artificial Intelligence for Assessment?</a>
  </h3>

  
  <div class="article-style">
    <p>Catheters are the second most common abnormal findings on radiographs. The position of catheters must be assessed on all radiographs, as serious complications can arise if catheters are malpositioned. However, due to the large number of radiographs performed each day, there can be substantial delays between the time a radiograph is performed and when it is interpreted by a radiologist. Computer-aided approaches hold the potential to assist in prioritizing radiographs with potentially malpositioned catheters for interpretation and automatically insert text indicating the placement of catheters in radiology reports, thereby improving radiologists’ efficiency.  Surprisingly, after 50 years of research in computer-aided diagnosis, there is still a paucity of study in this area. By carefully surveying the literature, we were only able to find 13 studies related to this task. Now, in the era of machine learning, or more specifically deep learning, the problem of catheter assessment is far more solvable. Therefore, we have performed a review of current algorithms and identified key challenges in building a reliable computer-aided diagnosis system for assessment of catheters on radiographs. This review may serve to further the development of machine learning approaches for this important use case.</p>
  </div>
  

  
  <div class="btn-links">
    








  







  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/xinario/PediatricXray100" target="_blank" rel="noopener">
  Dataset
</a>



  
    
  




  





  
  
  
    
  
  
  
  
  
    
    
      
    
  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/" >
    
    Comming soon
  </a>


  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2019">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    



  
  <span><a href="/authors/xin-yi/">Xin Yi</a></span>, <span><a href="/authors/ekta-walia/">Ekta Walia</a></span>, <span><a href="/authors/paul-babyn/">Paul Babyn</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    November 2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>MedIA</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/journal-article/gan_review/">Generative Adversarial Network in Medical Imaging: A Review</a>
  </h3>

  
  <div class="article-style">
    <p>Generative adversarial networks have gained a lot of attention in general computer vision community due to their capability of data generation without explicitly modelling the probability density function and robustness to overfitting. The adversarial loss brought by the discriminator provides a clever way of incorporating unlabeled samples into the training and imposing higher order consistency that is proven to be useful in many cases, such as in domain adaptation, data augmentation, and image-to-image translation. These nice properties have attracted researcher in the medical imaging community and we have seen quick adoptions in many traditional tasks and some novel applications. This trend will continue to grow based on our observation, therefore we conducted a review of the recent advances in medical imaging using the adversarial training scheme in the hope of benefiting researchers that are interested in this technique.</p>
  </div>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://www.sciencedirect.com/science/article/abs/pii/S1361841518308430" target="_blank" rel="noopener">
  PDF
</a>







  
    
  




  





  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/1809.07294" target="_blank" rel="noopener">
    
    preprint
  </a>


  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2019">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    



  
  <span><a href="/authors/xin-yi/">Xin Yi</a></span>, <span><a href="/authors/scott-adams/">Scott Adams</a></span>, <span><a href="/authors/abdul-elnajmi/">Abdul Elnajmi</a></span>, <span><a href="/authors/paul-babyn/">Paul Babyn</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    March 2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>J Digit Imaging</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/journal-article/catheter_detection/">Automatic Catheter and Tube Detection in Pediatric X-ray Images Using a Scale-Recurrent Network and Synthetic Data</a>
  </h3>

  
  <div class="article-style">
    <p>Catheters are commonly inserted life supporting devices. Because serious complications can arise from malpositioned catheters, X-ray images are used to assess the position of a catheter immediately after placement. Previous computer vision approaches to detect catheters on X-ray images were either rule-based or only capable of processing a limited number or type of catheters projecting over the chest. With the resurgence of deep learning, supervised training approaches are beginning to show promising results. However, dense annotation maps are required, and the work of a human annotator is difficult to scale. In this work, we propose an automatic approach for detection of catheters and tubes on pediatric X-ray images. We propose a simple way of synthesizing catheters on X-ray images to generate a training dataset by exploiting the fact that catheters are essentially tubular structures with various cross sectional profiles. Further, we develop a UNet-style segmentation network with a recurrent module that can process inputs at multiple scales and iteratively refine the detection result. By training on adult chest X-rays, the proposed network exhibits promising detection results on pediatric chest/abdomen X-rays in terms of both precision and recall, with Fβ = 0.8. The approach described in this work may contribute to the development of clinical systems to detect and assess the placement of catheters on X-ray images. This may provide a solution to triage and prioritize X-ray images with potentially malpositioned catheters for a radiologist’s urgent review and help automate radiology reporting.</p>
  </div>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/%22" target="_blank" rel="noopener">
  PDF
</a>




  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/xinario/catheter_detection" target="_blank" rel="noopener">
  Code
</a>




  
    
  



  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/files/poster_srcnn.pdf" target="_blank" rel="noopener">
  Poster
</a>


  





  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://openreview.net/forum?id=By47mM_oG" target="_blank" rel="noopener">
    
    preprint
  </a>


  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2018">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    



  
  <span><a href="/authors/xin-yi/">Xin Yi</a></span>, <span><a href="/authors/paul-babyn/">Paul Babyn</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    March 2018
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/preprint/dermoscopy/">Unsupervised and semi-supervised learning with Categorical Generative Adversarial Networks assisted by Wasserstein distance for dermoscopy image Classification</a>
  </h3>

  
  <div class="article-style">
    <p>Melanoma is a curable aggressive skin cancer if detected early. Typically, the diagnosis involves initial screening with subsequent biopsy and histopathological examination if necessary. Computer aided diagnosis offers an objective score that is independent of clinical experience and the potential to lower the workload of a dermatologist. In the recent past, success of deep learning algorithms in the field of general computer vision has motivated successful application of supervised deep learning methods in computer aided melanoma recognition. However, large quantities of labeled images are required to make further improvements on the supervised method. A good annotation generally requires clinical and histological confirmation, which requires significant effort. In an attempt to alleviate this constraint, we propose to use categorical generative adversarial network to automatically learn the feature representation of dermoscopy images in an unsupervised and semi-supervised manner. Thorough experiments on ISIC 2016 skin lesion challenge demonstrate that the proposed feature learning method has achieved an average precision score of 0.424 with only 140 labeled images. Moreover, the proposed method is also capable of generating real-world like dermoscopy images.</p>
  </div>
  

  
  <div class="btn-links">
    








  









  
    
  




  





  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/1804.03700" target="_blank" rel="noopener">
    
    preprint
  </a>


  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2018">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    



  
  <span><a href="/authors/xin-yi/">Xin Yi</a></span>, <span><a href="/authors/paul-babyn/">Paul Babyn</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    March 2018
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>J Digit Imaging</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/journal-article/sagan/">Sharpness-aware low dose CT denoising using conditional generative adversarial network</a>
  </h3>

  
  <div class="article-style">
    <p>Low Dose Computed Tomography (LDCT) has offered tremendous benefits in radiation restricted applications, but the quantum noise as resulted by the insufficient number of photons could potentially harm the diagnostic performance. Current image-based denoising methods tend to produce a blur effect on the final reconstructed results especially in high noise levels. In this paper, a deep learning based approach was proposed to mitigate this problem. An adversarially trained network and a sharpness detection network were trained to guide the training process. Experiments on both simulated and real dataset shows that the results of the proposed method have very small resolution loss and achieves better performance relative to the-state-of-art methods both quantitatively and visually.</p>
  </div>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://link.springer.com/article/10.1007/s10278-018-0056-0" target="_blank" rel="noopener">
  PDF
</a>




  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/xinario/SAGAN" target="_blank" rel="noopener">
  Code
</a>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://docs.google.com/forms/d/e/1FAIpQLSdnIxHU-hX_fDC4FSqnTe_BbvF3MR4N1hkIMT0T5SHwcmqP0Q/viewform" target="_blank" rel="noopener">
  Dataset
</a>



  
    
  




  





  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="http://arxiv.org/pdf/1512.04133v1" target="_blank" rel="noopener">
    
    preprint
  </a>


  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2016">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    



  
  <span><a href="/authors/xin-yi/">Xin Yi</a></span>, <span><a href="/authors/mark-eramian/">Mark Eramian</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    February 2016
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>TIP</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/journal-article/defocus_segmentation/">LBP-based Segmentation of Defocus Blur</a>
  </h3>

  
  <div class="article-style">
    <p>Defocus blur is extremely common in images captured using optical imaging systems. It may be undesirable, but may also be an intentional artistic effect, thus, it can either enhance or inhibit our visual perception of the image scene. For tasks such as image restoration and object recognition, one might want to segment a partially blurred image into blurred and non-blurred regions. In this paper, we propose a sharpness metric based on LBP (local binary patterns) and a robust segmentation algorithm to separate in- and out-of-focus image regions. The proposed sharpness metric exploits the observation that most local image patches in blurry regions have significantly fewer of certain local binary patterns compared to those in sharp regions. Using this metric together with image matting and multi-scale inference, we obtained high quality sharpness maps. Tests on hundreds of partially blurred images were used to evaluate our blur segmentation algorithm and six comparator methods. The results show that our algorithm achieves comparative segmentation results with the state-of-the-art and have big speed advantage over the others.</p>
  </div>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/abstract/document/7403985" target="_blank" rel="noopener">
  PDF
</a>




  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/xinario/defocus_segmentation" target="_blank" rel="noopener">
  Code
</a>




  
    
  




  





  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://www.cs.usask.ca/faculty/eramian/defocusseg/defocusSeg.pdf" target="_blank" rel="noopener">
    
    preprint
  </a>


  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2014">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    



  
  <span><a href="/authors/xin-yi/">Xin Yi</a></span>, <span><a href="/authors/mark-eramian/">Mark Eramian</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    May 2014
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>CRV2014</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/conference-paper/seed_crv/">Identification of morphologically similar seeds using multi-kernel learning</a>
  </h3>

  
  <div class="article-style">
    <p>Use of digital image analysis for the identification of seeds has not been recognized as a validated method. Image analysis for seed identification has been previously studied, and good recognition rates have been achieved. However, the data sets used in these experiments either contain very few groups of non-verified specimens or little representation of intra-species variations. This study considered a data set containing seed specimens that were verified to represent the species and a typical population variation, as well as look-alike species that share the same morphological appearance, in particular, seeds from species in the same genus, which can be particularly difficult for even trained professionals to visually distinguish. With representative specimens, the image features and machine learning algorithms described herein can achieve a high recognition rate: &gt;97%. Three different types of features from seed images: colour, shape, and texture were extracted, and a multi-kernel support vector machine was used as the classifier. We compared our features to the previous state-of-the-art features and the results showed that the features we selected performed better on our data set.</p>
  </div>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="http://ieeexplore.ieee.org/document/6816836/" target="_blank" rel="noopener">
  PDF
</a>







  
    
  




  






  </div>
  

</div>

          
        </div>

        
      </div>

    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.a0d331bcd05dbe8b31e244f796710f08.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
